{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LongCat Interactive Inference Notebook\n",
        "\n",
        "This notebook enables persistent model loading for LongCat video generation. \n",
        "Models are loaded once and remain in VRAM, allowing multiple inference runs without reloading.\n",
        "\n",
        "## Usage\n",
        "1. **Cell 1 (Setup)**: Run once to import libraries and define helper functions.\n",
        "2. **Cell 2 (Model Loading)**: Run once to load all models into VRAM (~40GB).\n",
        "3. **Cell 3 (Inference)**: Run repeatedly with different parameters. Supports batch inference and optional 720p upscaling.\n",
        "\n",
        "## Data Directory Structure\n",
        "```\n",
        "data/scene_name/\n",
        "├── imgs/              # Input frames and masks (VIDEO_REF_DIR points here)\n",
        "│   ├── 00000.png      # Video frames\n",
        "│   ├── 00001.png\n",
        "│   ├── mask_00000.png # Binary masks (white=inpaint region)\n",
        "│   └── mask_00001.png\n",
        "└── ref/               # (Optional) High-res reference frame for 720p upscaling\n",
        "    └── 00000.png\n",
        "```\n",
        "\n",
        "## Notes\n",
        "- Requires a single GPU with at least 40GB VRAM (e.g., A100/H100/A800).\n",
        "- First inference may be slower due to CUDA kernel compilation.\n",
        "- Modify `CHECKPOINT_DIR` in Cell 2 to point to your model weights directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Imports & Setup (run once)\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project directory to Python path\n",
        "PROJECT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))  # auto-detect project root\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "import datetime\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import torchvision.io as tvio\n",
        "\n",
        "from transformers import AutoTokenizer, UMT5EncoderModel\n",
        "from torchvision.io import write_video\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "from longcat_video.pipeline_longcat_video import LongCatVideoPipeline\n",
        "from prompts import get_prompt, list_available_scenes\n",
        "from longcat_video.modules.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler\n",
        "from longcat_video.modules.autoencoder_kl_wan import AutoencoderKLWan\n",
        "from longcat_video.modules.longcat_video_dit import LongCatVideoTransformer3DModel\n",
        "from longcat_video.context_parallel import context_parallel_util\n",
        "from longcat_video.context_parallel.context_parallel_util import init_context_parallel\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "def torch_gc():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "def load_video(video_path):\n",
        "    \"\"\"Load video file and return as list of PIL Images.\"\"\"\n",
        "    video, audio, info = tvio.read_video(video_path, pts_unit='sec')\n",
        "    return [PIL.Image.fromarray(video[i].numpy()) for i in range(video.shape[0])]\n",
        "\n",
        "def read_frames_from_directory(directory):\n",
        "    \"\"\"Read video frames and corresponding masks from a directory.\n",
        "    \n",
        "    Frames: regular image files (e.g., 00000.png)\n",
        "    Masks: files prefixed with 'mask_' (e.g., mask_00000.png)\n",
        "    \"\"\"\n",
        "    print(f\"Reading frames from: {directory}\")\n",
        "    \n",
        "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']\n",
        "    all_files = []\n",
        "    for ext in image_extensions:\n",
        "        all_files.extend(glob.glob(os.path.join(directory, ext)))\n",
        "    all_files = sorted(all_files)\n",
        "    \n",
        "    if not all_files:\n",
        "        raise ValueError(f\"No images found in {directory}\")\n",
        "    \n",
        "    frame_files = [f for f in all_files if not os.path.basename(f).startswith('mask_')]\n",
        "    mask_files = [f for f in all_files if os.path.basename(f).startswith('mask_')]\n",
        "    \n",
        "    frames = [PIL.Image.open(f).convert('RGB') for f in frame_files]\n",
        "    masks = [PIL.Image.open(f).convert('L') for f in mask_files]\n",
        "    \n",
        "    if not masks and frames:\n",
        "        print(\"No mask images found, creating zero masks\")\n",
        "        masks = [PIL.Image.new('L', frames[0].size, 0) for _ in frames]\n",
        "    \n",
        "    if len(masks) != len(frames):\n",
        "        print(f\"Warning: mask count ({len(masks)}) != frame count ({len(frames)})\")\n",
        "        while len(masks) < len(frames):\n",
        "            masks.append(masks[-1] if masks else PIL.Image.new('L', frames[0].size, 0))\n",
        "        masks = masks[:len(frames)]\n",
        "    \n",
        "    first_frame = frames[0] if frames else None\n",
        "    first_frame_path = frame_files[0] if frame_files else None\n",
        "    print(f\"Loaded {len(frames)} frames and {len(masks)} masks\")\n",
        "    return frames, masks, first_frame, first_frame_path\n",
        "\n",
        "def soften_mask(mask_array, transition_distance=15, decay_type='sine'):\n",
        "    \"\"\"Soften mask boundaries with smooth distance-based transitions.\"\"\"\n",
        "    softened_mask = mask_array.copy().astype(np.float32)\n",
        "    \n",
        "    for frame_idx in range(mask_array.shape[0]):\n",
        "        current_mask = mask_array[frame_idx].astype(bool)\n",
        "        if np.all(current_mask) or np.all(~current_mask):\n",
        "            continue\n",
        "        \n",
        "        softened_frame = mask_array[frame_idx].copy().astype(np.float32)\n",
        "        distance_from_ones = distance_transform_edt(current_mask)\n",
        "        ones_transition = current_mask & (distance_from_ones <= transition_distance)\n",
        "        \n",
        "        def smooth_transition(t, dtype):\n",
        "            t = np.clip(t, 0.0, 1.0)\n",
        "            if dtype == 'linear':    return t\n",
        "            elif dtype == 'exponential': return 1.0 - np.exp(-3.0 * t)\n",
        "            elif dtype == 'sine':    return np.sin(np.pi / 2 * t)\n",
        "            elif dtype == 'cosine':  return 1.0 - np.cos(np.pi / 2 * t)\n",
        "            else: raise ValueError(f\"Unsupported decay type: {dtype}\")\n",
        "        \n",
        "        if np.any(ones_transition):\n",
        "            distances = distance_from_ones[ones_transition]\n",
        "            softened_frame[ones_transition] = smooth_transition(distances / transition_distance, decay_type)\n",
        "        softened_mask[frame_idx] = softened_frame\n",
        "    \n",
        "    return softened_mask\n",
        "\n",
        "print(\"Cell 1 complete: environment configured, helper functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 2: Model Loading (run once)\n",
        "# ============================================================\n",
        "# Models will remain in VRAM after this cell completes.\n",
        "\n",
        "# ==================== Configuration ====================\n",
        "CHECKPOINT_DIR = \"/path/to/LongCat-Video\"  # Path to model weights\n",
        "CONTEXT_PARALLEL_SIZE = 1\n",
        "ENABLE_COMPILE = False    # Enable torch.compile (slower first run)\n",
        "USE_DISTILL = False       # Use 16-step distillation mode\n",
        "# =======================================================\n",
        "\n",
        "# Performance settings\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "if hasattr(torch._dynamo.config, 'cache_size_limit'):\n",
        "    torch._dynamo.config.cache_size_limit = 128\n",
        "\n",
        "# Single GPU setup\n",
        "local_rank = 0\n",
        "global_rank = 0\n",
        "num_processes = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(local_rank)\n",
        "    print(f\"Using GPU {local_rank}: {torch.cuda.get_device_name(local_rank)}\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA is not available.\")\n",
        "\n",
        "# Initialize distributed environment (single-process mode)\n",
        "if not dist.is_initialized():\n",
        "    import socket\n",
        "    def find_free_port():\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.bind(('', 0))\n",
        "            s.listen(1)\n",
        "            return s.getsockname()[1]\n",
        "    \n",
        "    master_port = str(find_free_port())\n",
        "    os.environ.update({\n",
        "        'MASTER_ADDR': 'localhost', 'MASTER_PORT': master_port,\n",
        "        'RANK': '0', 'LOCAL_RANK': '0', 'WORLD_SIZE': '1'\n",
        "    })\n",
        "    dist.init_process_group(\n",
        "        backend=\"nccl\", init_method=f\"tcp://localhost:{master_port}\",\n",
        "        rank=0, world_size=1, timeout=datetime.timedelta(seconds=3600)\n",
        "    )\n",
        "\n",
        "# Initialize context parallel\n",
        "init_context_parallel(context_parallel_size=CONTEXT_PARALLEL_SIZE, global_rank=global_rank, world_size=num_processes)\n",
        "cp_size = context_parallel_util.get_cp_size()\n",
        "cp_split_hw = context_parallel_util.get_optimal_split(cp_size)\n",
        "\n",
        "# Load models (suppress harmless LOAD REPORT from transformers 5.x about T5 weight-tying)\n",
        "print(\"Loading models...\")\n",
        "import transformers as _tf; _prev_verbosity = _tf.logging.get_verbosity(); _tf.logging.set_verbosity_error()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR, subfolder=\"tokenizer\")\n",
        "text_encoder = UMT5EncoderModel.from_pretrained(CHECKPOINT_DIR, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
        "print(\"Tokenizer + Text encoder loaded\")\n",
        "\n",
        "vae = AutoencoderKLWan.from_pretrained(CHECKPOINT_DIR, subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
        "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(CHECKPOINT_DIR, subfolder=\"scheduler\")\n",
        "print(\"VAE + Scheduler loaded\")\n",
        "\n",
        "dit = LongCatVideoTransformer3DModel.from_pretrained(CHECKPOINT_DIR, subfolder=\"dit\", cp_split_hw=cp_split_hw, torch_dtype=torch.bfloat16)\n",
        "print(\"DiT model loaded\")\n",
        "_tf.logging.set_verbosity(_prev_verbosity)\n",
        "\n",
        "# Pre-load all LoRAs (loaded once, toggle enable/disable later)\n",
        "cfg_step_lora_path = os.path.join(CHECKPOINT_DIR, 'lora/cfg_step_lora.safetensors')\n",
        "dit.load_lora(cfg_step_lora_path, 'cfg_step_lora')\n",
        "\n",
        "refinement_lora_path = os.path.join(CHECKPOINT_DIR, 'lora/refinement_lora.safetensors')\n",
        "dit.load_lora(refinement_lora_path, 'refinement_lora')\n",
        "print(\"LoRAs loaded (cfg_step_lora + refinement_lora)\")\n",
        "\n",
        "if USE_DISTILL:\n",
        "    dit.enable_loras(['cfg_step_lora'])\n",
        "    print(\"Distill mode: cfg_step_lora enabled\")\n",
        "else:\n",
        "    dit.disable_all_loras()\n",
        "    print(\"Standard mode: all LoRAs disabled\")\n",
        "\n",
        "if ENABLE_COMPILE:\n",
        "    print(\"Compiling DiT model...\")\n",
        "    dit = torch.compile(dit)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = LongCatVideoPipeline(\n",
        "    tokenizer=tokenizer, text_encoder=text_encoder,\n",
        "    vae=vae, scheduler=scheduler, dit=dit,\n",
        ")\n",
        "pipe.to(local_rank)\n",
        "\n",
        "print(f\"\\nModels loaded. VRAM: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Batch Inference (run repeatedly with different params)\n",
        "# ============================================================\n",
        "\n",
        "# ==================== Parameters (modify as needed) ====================\n",
        "VIDEO_REF_DIR = \"/path/to/data/scene_name/imgs\"   # Directory with frames + mask_ files\n",
        "OUTPUT_BASE_DIR = \"/path/to/output\"               # Output base directory\n",
        "SCENE_NAME = \"coffee\"            # Scene name (for prompt lookup, see prompts.py)\n",
        "RESOLUTION = \"480p\"             # Inference resolution\n",
        "NUM_FRAMES = 29                 # Number of frames to generate\n",
        "STATIC_MODE = False             # Static scene mode (less motion)\n",
        "FPS = 16                        # Output video FPS\n",
        "NUM_INFERENCE_STEPS = 50        # Diffusion steps (16 for distill, 50 for standard)\n",
        "\n",
        "# Parameter grid for batch inference\n",
        "max_channels_list = [1]         # Max FLF replacement channels\n",
        "guidance_scales_list = [4]      # CFG scale\n",
        "omegas_list = [4]               # Auto-guidance omega\n",
        "transition_distances_list = [15] # Mask softening distance (0=no softening)\n",
        "step_additions_list = [0]       # Addition to guide_steps for resample_round\n",
        "step_guide_list = [20]          # Guide steps\n",
        "seeds_list = [42]               # Random seeds\n",
        "\n",
        "# Fixed parameters\n",
        "RESAMPLE_STEPS = 2\n",
        "USE_PCA_CHANNEL_SELECTION = True\n",
        "ENABLE_SOFTEN_MASK = True\n",
        "DECAY_TYPE = \"sine\"\n",
        "SAVE_PNG = False\n",
        "\n",
        "# Upscaling parameters\n",
        "ENABLE_UPSCALE = True          # Enable 720p upscaling\n",
        "T_THRESH = 0.6                 # Upscale denoise threshold (0.0-1.0)\n",
        "# =======================================================================\n",
        "\n",
        "import itertools\n",
        "\n",
        "param_combinations = list(itertools.product(\n",
        "    guidance_scales_list, max_channels_list, transition_distances_list,\n",
        "    step_additions_list, omegas_list, step_guide_list, seeds_list\n",
        "))\n",
        "\n",
        "print(f\"Total {len(param_combinations)} parameter combinations\")\n",
        "if ENABLE_UPSCALE:\n",
        "    print(f\"Upscaling: enabled (t_thresh={T_THRESH})\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def read_ref_frame_for_upscale(video_ref_dir):\n",
        "    \"\"\"Read the first high-res reference frame from ref/ directory (sibling of imgs/).\"\"\"\n",
        "    ref_dir = os.path.join(os.path.dirname(video_ref_dir), \"ref\")\n",
        "    if not os.path.exists(ref_dir):\n",
        "        print(f\"Warning: ref directory not found: {ref_dir}\")\n",
        "        return None, None\n",
        "    \n",
        "    ref_files = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:\n",
        "        ref_files.extend(glob.glob(os.path.join(ref_dir, ext)))\n",
        "    ref_files = sorted(ref_files)\n",
        "    \n",
        "    if not ref_files:\n",
        "        print(f\"Warning: No reference images found in {ref_dir}\")\n",
        "        return None, None\n",
        "    \n",
        "    ref_frame = PIL.Image.open(ref_files[0]).convert('RGB')\n",
        "    print(f\"Loaded reference frame: {ref_files[0]} ({ref_frame.size[0]}x{ref_frame.size[1]})\")\n",
        "    return ref_frame, ref_files[0]\n",
        "\n",
        "def run_single_inference(params):\n",
        "    g, mc, m, addition, o, step, seed_val = params\n",
        "    r_r = step + addition\n",
        "    \n",
        "    # Build output filename\n",
        "    distill_suffix = \"dt16\" if USE_DISTILL else \"fu50\"\n",
        "    output_file = f\"{OUTPUT_BASE_DIR}/{distill_suffix}/{RESOLUTION}/o{o}_re{RESAMPLE_STEPS}_guide{step}_round{r_r}_mask{m}_max{mc}_seed{seed_val}.mp4\"\n",
        "    \n",
        "    # Skip if already exists\n",
        "    if os.path.exists(output_file):\n",
        "        if ENABLE_UPSCALE:\n",
        "            output_720p_file = f\"{os.path.splitext(output_file)[0]}_720p.mp4\"\n",
        "            if os.path.exists(output_720p_file):\n",
        "                print(f\"Skipping (exists): {os.path.basename(output_file)}\")\n",
        "                return True\n",
        "        else:\n",
        "            print(f\"Skipping (exists): {os.path.basename(output_file)}\")\n",
        "            return True\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"omega={o}, resample={RESAMPLE_STEPS}, guide_steps={step}, round={r_r}, mask={m}, max_replace={mc}, seed={seed_val}\")\n",
        "    print(f\"Output: {output_file}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    try:\n",
        "        frames, masks, first_frame, _ = read_frames_from_directory(VIDEO_REF_DIR)\n",
        "        \n",
        "        image = first_frame\n",
        "        if image is None:\n",
        "            raise ValueError(\"No first frame available\")\n",
        "        \n",
        "        # Compute target dimensions\n",
        "        scale_factor_spatial = pipe.vae_scale_factor_spatial * 2\n",
        "        if pipe.dit.cp_split_hw is not None:\n",
        "            scale_factor_spatial *= max(pipe.dit.cp_split_hw)\n",
        "        height, width = pipe.get_condition_shape(image, RESOLUTION, scale_factor_spatial=scale_factor_spatial)\n",
        "        \n",
        "        # Process video frames and masks\n",
        "        resized_frames = [frame.resize((width, height)) for frame in frames]\n",
        "        video_frames = torch.stack([\n",
        "            torch.tensor(np.array(frame)).permute(2, 0, 1).float() / 255.0\n",
        "            for frame in resized_frames\n",
        "        ])\n",
        "        video_ref = video_frames.unsqueeze(0).permute(0, 2, 1, 3, 4)\n",
        "        \n",
        "        mask = None\n",
        "        if masks:\n",
        "            resized_masks = [mk.resize((width, height)) for mk in masks]\n",
        "            mask_array = np.stack([np.array(mk) / 255.0 for mk in resized_masks])\n",
        "            if ENABLE_SOFTEN_MASK and m > 0:\n",
        "                mask_array = soften_mask(mask_array, m, DECAY_TYPE)\n",
        "            mask = torch.from_numpy(mask_array).unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        prompt = get_prompt(SCENE_NAME)\n",
        "        negative_prompt = (\n",
        "            \"Blink, twinkle, waggle, speak, wind, windy, leaves shaking, leaves tremble, \"\n",
        "            \"background dynamics, dynamic imagery, gray sky, hazy sky, overcast, \"\n",
        "            \"gloomy sky, dim, murky, smoggy, shake, object motion blur, streaking objects, \"\n",
        "            \"object jitter, camera shake, illogical composition, bright tones, \"\n",
        "            \"overexposed, blurred details, subtitles, text, logo, worst quality, \"\n",
        "            \"low quality, ugly, incomplete, sudden scene shift, incoherent scene jump, \"\n",
        "            \"extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, \"\n",
        "            \"misshapen limbs, fused fingers, any movement, character motion, \"\n",
        "            \"object vibration, messy background, scene changes, object disintegration.\"\n",
        "        ) if STATIC_MODE else (\n",
        "            \"Streaking objects, mosaic, grainy, pixelated, noise, flickering, cropped, glitch, \"\n",
        "            \"fragmented, broken, artifacts, chromatic aberration, camera shake, \"\n",
        "            \"blurry, sudden scene shift, incoherent scene jump, sudden object appearance, \"\n",
        "            \"blinking, object jitter, illogical composition, bright tones, overexposed, \"\n",
        "            \"blurred details, subtitles, worst quality, low quality, \"\n",
        "            \"ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, \"\n",
        "            \"deformed, disfigured, misshapen limbs, fused fingers, messy background\"\n",
        "        )\n",
        "        \n",
        "        generator = torch.Generator(device='cpu')\n",
        "        generator.manual_seed(seed_val)\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        \n",
        "        # Run 480p inference\n",
        "        output = pipe.generate_i2v(\n",
        "            image=image,\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            resolution=RESOLUTION,\n",
        "            num_frames=NUM_FRAMES,\n",
        "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
        "            use_distill=USE_DISTILL,\n",
        "            guidance_scale=1.0 if USE_DISTILL else g,\n",
        "            num_videos_per_prompt=1,\n",
        "            generator=generator,\n",
        "            video_ref=video_ref,\n",
        "            mask=mask,\n",
        "            guided=True,\n",
        "            resample_steps=RESAMPLE_STEPS,\n",
        "            guide_steps=step,\n",
        "            resample_round=r_r,\n",
        "            omega=o,\n",
        "            omega_resample=o,\n",
        "            use_pca_channel_selection=USE_PCA_CHANNEL_SELECTION,\n",
        "            static=STATIC_MODE,\n",
        "            max_replace_threshold=mc,\n",
        "        )[0]\n",
        "        \n",
        "        # Save 480p video\n",
        "        output_pil = [PIL.Image.fromarray((output[i] * 255).astype(np.uint8)) for i in range(output.shape[0])]\n",
        "        output_tensor = torch.from_numpy(np.array([np.array(img) for img in output_pil]))\n",
        "        write_video(output_file, output_tensor, fps=FPS, video_codec=\"libx264\", options={\"crf\": \"10\"})\n",
        "        \n",
        "        if SAVE_PNG:\n",
        "            png_dir = os.path.join(os.path.dirname(output_file), f\"imgs_{os.path.splitext(os.path.basename(output_file))[0]}\")\n",
        "            os.makedirs(png_dir, exist_ok=True)\n",
        "            for i, frame in enumerate(output_pil):\n",
        "                frame.save(os.path.join(png_dir, f\"{i:05d}.png\"))\n",
        "        \n",
        "        print(f\"480p done: {os.path.basename(output_file)}\")\n",
        "        \n",
        "        # Optional 720p upscaling\n",
        "        if ENABLE_UPSCALE:\n",
        "            print(\"Starting 720p upscaling...\")\n",
        "            output_720p_file = f\"{os.path.splitext(output_file)[0]}_720p.mp4\"\n",
        "            \n",
        "            ref_frame, _ = read_ref_frame_for_upscale(VIDEO_REF_DIR)\n",
        "            if ref_frame is None:\n",
        "                print(\"Warning: no reference frame found, skipping upscale\")\n",
        "            else:\n",
        "                # Switch to refinement LoRA\n",
        "                pipe.dit.disable_all_loras()\n",
        "                pipe.dit.enable_loras(['refinement_lora'])\n",
        "                pipe.dit.enable_bsa()\n",
        "                \n",
        "                video_frames_480p = load_video(output_file)\n",
        "                print(f\"Loaded {len(video_frames_480p)} frames for upscaling\")\n",
        "                \n",
        "                # Compute model-compatible target resolution\n",
        "                ref_width, ref_height = ref_frame.size\n",
        "                sf = pipe.vae_scale_factor_spatial * 2\n",
        "                if pipe.dit.cp_split_hw is not None:\n",
        "                    sf *= max(pipe.dit.cp_split_hw)\n",
        "                target_h = (ref_height // sf) * sf\n",
        "                target_w = (ref_width // sf) * sf\n",
        "                ref_frame_resized = ref_frame.resize((target_w, target_h))\n",
        "                print(f\"Upscale target: {target_w}x{target_h}\")\n",
        "                \n",
        "                generator_upscale = torch.Generator(device='cpu')\n",
        "                generator_upscale.manual_seed(seed_val)\n",
        "                \n",
        "                output_refine = pipe.generate_refine(\n",
        "                    image=ref_frame_resized,\n",
        "                    prompt=prompt,\n",
        "                    stage1_video=video_frames_480p,\n",
        "                    num_cond_frames=1,\n",
        "                    num_inference_steps=50,\n",
        "                    generator=generator_upscale,\n",
        "                    spatial_refine_only=True,\n",
        "                    t_thresh=T_THRESH,\n",
        "                )[0]\n",
        "                \n",
        "                # Restore LoRA state\n",
        "                pipe.dit.disable_all_loras()\n",
        "                pipe.dit.disable_bsa()\n",
        "                if USE_DISTILL:\n",
        "                    pipe.dit.enable_loras(['cfg_step_lora'])\n",
        "                \n",
        "                # Save 720p video\n",
        "                output_720p_frames = [(output_refine[i] * 255).astype(np.uint8) for i in range(output_refine.shape[0])]\n",
        "                output_720p_tensor = torch.from_numpy(np.array(output_720p_frames))\n",
        "                write_video(output_720p_file, output_720p_tensor, fps=FPS, video_codec=\"libx264\", options={\"crf\": \"10\"})\n",
        "                \n",
        "                if SAVE_PNG:\n",
        "                    png_720p_dir = os.path.join(os.path.dirname(output_720p_file), f\"imgs_{os.path.splitext(os.path.basename(output_720p_file))[0]}\")\n",
        "                    os.makedirs(png_720p_dir, exist_ok=True)\n",
        "                    for i, frame_data in enumerate(output_720p_frames):\n",
        "                        PIL.Image.fromarray(frame_data).save(os.path.join(png_720p_dir, f\"{i:05d}.png\"))\n",
        "                \n",
        "                print(f\"720p done: {os.path.basename(output_720p_file)}\")\n",
        "        \n",
        "        torch_gc()\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        torch_gc()\n",
        "        return False\n",
        "\n",
        "# Run batch inference\n",
        "print(f\"\\nStarting batch inference...\")\n",
        "success_count = 0\n",
        "fail_count = 0\n",
        "\n",
        "for idx, params in enumerate(param_combinations):\n",
        "    print(f\"\\n[{idx+1}/{len(param_combinations)}] \", end=\"\")\n",
        "    if run_single_inference(params):\n",
        "        success_count += 1\n",
        "    else:\n",
        "        fail_count += 1\n",
        "\n",
        "print(f\"\\nBatch complete: {success_count} success, {fail_count} failed. VRAM: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "longcat",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
